```
layout: post
title: "How Wikipedia Reacts to Breaking News: Exploring the WAVE Project"
author: Simeon Fl√ºhmann, Joanna Gutbrod & Elias Hager
```

## Capturing the Flow of News into Wikipedia

Have you ever noticed how quickly Wikipedia seems to reflect major news events? Within days of a headline breaking, articles are updated, edited and expanded. But how exactly does that happen? The WAVE engine (short for Wikipedia Analysis and Verification Engine) was developed to trace this process, from published news articles to visible Wikipedia edits.

**Try it out yourself here:** [wave.fluehmann.earth](https://wave.fluehmann.earth/)

WAVE is not a real-time tool. It operates with a delay because the Swissdox API, its main news source, only provides articles two days after publication. This makes WAVE unsuitable for live monitoring, but it is well suited to studying how Wikipedia reacts to verified news coverage in a structured way.

## Connecting News to Wikipedia

Our system fetches news articles from the Swissdox API for a specified date These articles are grouped by topic, summarised, and linked to relevant German Wikipedia pages. To achieve this, we tested three different approaches:

### TEXT Approach

Articles are converted into semantic vectors using the `all-MiniLM-L12-v2` model, combining headline and body. DBSCAN then clusters related articles. Each cluster is sent to the `LLaMA-3.3-8b-8192` model via Groq to:

1. Generate a summary
2. Suggest Wikipedia titles

Suggested titles are validated with the Wikipedia API and saved if they align with the summary.

### SUM Approach

This approach uses the same clustering method as TEXT, but starts with cluster summaries. Based on these, the model suggests Wikipedia titles. Titles are validated in the same way.

### NER Approach

This method uses SpaCy to extract named entities, giving headline entities more weight than the body. Articles are vectorised with TF-IDF, and cosine similarity is used for clustering. Frequent entities guide Wikipedia title suggestions.

Each cluster is summarised with LLaMA, and suggested titles are validated via the Wikipedia API before being stored.

### Final Architecture

The final pipeline combines the best of both worlds: NER for clustering and SUM for Wikipedia linking and summary.

1. Articles are cleaned, deduplicated, and clustered based on weighted entities and TF-IDF similarity.
2. Each cluster is summarised using LLaMA on Groq.
3. Titles are derived from summaries and validated for existence and content match using the Wikipedia API.

Only verified titles and summaries are saved.

### Evaluation and Comparison

The team compared three clustering and linking strategies by evaluating thematic coherence, the accuracy of Wikipedia linking, and the quality of summaries. Human annotators scored each method on a scale from 1 to 5.

**Table 1: Median Evaluation Scores by Method**

| Criterion                       | TEXT | SUM  | NER  |
|---------------------------------|:----:|:----:|:----:|
| Thematic Relevance              | 4.0  | 5.0  | 5.0  |
| Relevance of Wikipedia Linking  | 3.0  | 4.5  | 3.5  |
| Summary Quality                 | 3.5  | 5.0  | 4.0  |

*Table 1 shows median scores on a scale from 1 (low) to 5 (high) for three evaluation criteria.*

## Key Results

The SUM approach, which generates summaries before clustering, delivered the most consistent results. It scored highest for both thematic relevance and summary quality, and its Wikipedia link suggestions were more reliable than those from other methods.

The NER approach performed especially well in capturing meaningful event clusters, particularly when articles referenced clear entities such as specific people or locations. However, its Wikipedia linking was slightly less consistent.

The TEXT approach was the fastest to compute, as it skipped summarisation and relied only on embeddings. While it worked in many cases, it sometimes grouped unrelated articles or linked to less relevant Wikipedia pages.

The final WAVE architecture combines the strengths of both the SUM and NER methods. Clustering is based on named entities, which keeps the groupings focused and coherent. Summaries are then generated using the SUM pipeline to provide clear context for linking. This hybrid setup has proven to be both effective and efficient.

## Looking Ahead

WAVE offers a valuable lens into how information flows from journalism into public knowledge repositories like Wikipedia. Although it does not operate in real time, the two-day delay still allows for detailed tracking of content evolution. Future improvements may include better handling of ambiguous names, support for additional languages and media types, and integration with other real-world knowledge sources.

Whether you are a researcher, a journalist or simply someone interested in how collective knowledge takes shape, WAVE provides an insightful and transparent way to explore that process.
